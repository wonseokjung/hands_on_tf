{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Tensorflow_models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/wonseokjung/hands_on_tf/blob/master/2_Tensorflow_models.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "4jdpLdM8-JLi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Models & Layers \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aJW9g9is-NDO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. We usually implement models as classes and use statements like y_pred( ) = model( x ) to call the models. \n",
        "\n",
        "2. The structure of a model class is rather simple which basically include __init__( ) ( for construction and initialization ) \n",
        "\n",
        "3. call(input ) while you can also define your own methods if necessary \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Kiw0phnVi1Sy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6qdTQ6O2-2nm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Call method에서 불려질 initialization code를 넣는다. \n",
        "    \n",
        "  def call(self, inputs): \n",
        "    \n",
        "    #여기에는 모델 코드를 넣는다. \n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nocw59LO-NHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our model inherits from tf.kears.Model.\n",
        "\n",
        "Keras is an advanced neural network API written in Python and is supperted by TensorFlow officially. \n",
        "\n",
        "One benefit of inheriting from tf.keras.Model is that we will be able to use several methods and attributes like acquiring all variables in the model through the model.variable attribute after the class is instantiated.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Haii9L0cS25u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**y_pred = tf.matmul(X,w) + b  can be implemented thorugh model classes **"
      ]
    },
    {
      "metadata": {
        "id": "d8CnqkNabAIl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# EXAMPLE : Mnist , Multilayer Perceptron \n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/afcyulmasb6cpp0/Screenshot%202018-09-30%2021.19.56.png?raw=1\">\n",
        "**bold text**"
      ]
    },
    {
      "metadata": {
        "id": "1MOtSRp_W7zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataLoader():\n",
        "  def __init__(self):\n",
        "    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "    \n",
        "    self.train_data = mnist.train.images \n",
        "    self.train_labels = np.asarray(mnist.train.labels, dtype= np.int32)\n",
        "    self.eval_data = mnist.test.images \n",
        "    self.eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
        "   \n",
        "\n",
        "  def get_batch(self, batch_size):\n",
        "    index = np.random.randint(0,np.shape(self.train_data)[0], batch_size)\n",
        "    \n",
        "    return self.train_data[index, :], self.train_labels[index]\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7eDkPinrW733",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP(tf.keras.Model): \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.dense1 = tf.keras.layers.Dense(units = 100, activation =tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(units = 10)\n",
        "    \n",
        "    \n",
        "    \n",
        "  def call(self, inputs):\n",
        "    \n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "    \n",
        "  def predict(self, inputs ):\n",
        "    \n",
        "    logits = self(inputs)\n",
        "    \n",
        "    return tf.argmax(logits, axis =-1)\n",
        "  \n",
        "  \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p8KjwZn-ieVN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define hyperparameters \n"
      ]
    },
    {
      "metadata": {
        "id": "j1lwq2SFW78q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_batches = 1000\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "learning_rate = 0.001 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vok7I4JhW7_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = MLP()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ndcK7e1iopf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "480a04ef-63a0-472f-e8f3-f54292986614"
      },
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader() \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xNqKyVCdwCNk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Iteration steps: \n",
        "\n",
        "1. DataLoader 를 통하여 training data의 set을  임의로 읽는다. \n",
        "\n",
        "2. data를 model로 feed한다. \n",
        "\n",
        "3. 정답과 prediction을 비교한다. ( loss function )\n",
        "\n",
        "4. Loss function을 Differentiate한다. \n",
        "\n",
        "5. Loss를 Minimize하기위해 model parameter을 업데이트한다. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DqBLqVFhiosX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17017
        },
        "outputId": "b94e5f53-a011-40b1-fd38-87331f2d58d7"
      },
      "cell_type": "code",
      "source": [
        "for batch_index in range(num_batches):\n",
        "  X,y =data_loader.get_batch(batch_size)\n",
        "  \n",
        "  with tf.GradientTape() as tape: \n",
        "    y_logit_pred = model(tf.convert_to_tensor(X))\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels= y, logits = y_logit_pred)\n",
        "    \n",
        "    print(batch_index,loss.numpy())\n",
        "    \n",
        "  grads = tape.gradient(loss, model.variables)\n",
        "  \n",
        "  optimizer.apply_gradients(grads_and_vars= zip(grads, model.variables))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.3836544\n",
            "1 2.3577263\n",
            "2 2.369278\n",
            "3 2.132346\n",
            "4 2.0906208\n",
            "5 2.0831938\n",
            "6 1.9190334\n",
            "7 1.9203488\n",
            "8 1.882431\n",
            "9 1.9033542\n",
            "10 1.7308813\n",
            "11 1.6847018\n",
            "12 1.7609009\n",
            "13 1.5581069\n",
            "14 1.5830148\n",
            "15 1.5604353\n",
            "16 1.4125894\n",
            "17 1.4222411\n",
            "18 1.3887985\n",
            "19 1.2715633\n",
            "20 1.2639377\n",
            "21 1.1925238\n",
            "22 1.2894301\n",
            "23 1.0862559\n",
            "24 1.177566\n",
            "25 0.95479125\n",
            "26 1.0635705\n",
            "27 0.90716416\n",
            "28 1.073618\n",
            "29 0.9138944\n",
            "30 0.89936185\n",
            "31 1.10321\n",
            "32 0.76090544\n",
            "33 0.78239065\n",
            "34 1.0342768\n",
            "35 0.86361104\n",
            "36 0.97453135\n",
            "37 0.767191\n",
            "38 0.7842591\n",
            "39 0.6515729\n",
            "40 0.94250923\n",
            "41 0.84330106\n",
            "42 0.82899356\n",
            "43 0.8824417\n",
            "44 0.71149206\n",
            "45 0.4530648\n",
            "46 0.693158\n",
            "47 0.6707597\n",
            "48 0.79366845\n",
            "49 0.7266124\n",
            "50 0.7087634\n",
            "51 0.7537384\n",
            "52 0.6567216\n",
            "53 0.53287256\n",
            "54 0.60967433\n",
            "55 0.6544329\n",
            "56 0.5755513\n",
            "57 0.5667496\n",
            "58 0.5499377\n",
            "59 0.43579963\n",
            "60 0.56829375\n",
            "61 0.4802512\n",
            "62 0.64409304\n",
            "63 0.46271706\n",
            "64 0.55420303\n",
            "65 0.46038255\n",
            "66 0.5470114\n",
            "67 0.6220205\n",
            "68 0.56924814\n",
            "69 0.4529014\n",
            "70 0.502906\n",
            "71 0.4191805\n",
            "72 0.5958595\n",
            "73 0.53185385\n",
            "74 0.5823629\n",
            "75 0.5250027\n",
            "76 0.62656236\n",
            "77 0.5083483\n",
            "78 0.3140443\n",
            "79 0.49648684\n",
            "80 0.7320845\n",
            "81 0.71378297\n",
            "82 0.35224614\n",
            "83 0.52449286\n",
            "84 0.4394031\n",
            "85 0.27020812\n",
            "86 0.6255697\n",
            "87 0.67853254\n",
            "88 0.44801843\n",
            "89 0.52002496\n",
            "90 0.581238\n",
            "91 0.312461\n",
            "92 0.516947\n",
            "93 0.43074104\n",
            "94 0.54736674\n",
            "95 0.2805149\n",
            "96 0.48626536\n",
            "97 0.42851874\n",
            "98 0.4843205\n",
            "99 0.5062129\n",
            "100 0.36328557\n",
            "101 0.46081054\n",
            "102 0.5569711\n",
            "103 0.5469186\n",
            "104 0.42719743\n",
            "105 0.34591752\n",
            "106 0.45694983\n",
            "107 0.2776498\n",
            "108 0.23661472\n",
            "109 0.47336465\n",
            "110 0.35532418\n",
            "111 0.7288957\n",
            "112 0.48840696\n",
            "113 0.29580414\n",
            "114 0.29832107\n",
            "115 0.42355987\n",
            "116 0.47929642\n",
            "117 0.4158002\n",
            "118 0.6774571\n",
            "119 0.5756049\n",
            "120 0.7334088\n",
            "121 0.35301363\n",
            "122 0.28124526\n",
            "123 0.42447266\n",
            "124 0.25236502\n",
            "125 0.48178712\n",
            "126 0.44582152\n",
            "127 0.36248899\n",
            "128 0.25917038\n",
            "129 0.39675748\n",
            "130 0.398835\n",
            "131 0.21863507\n",
            "132 0.5359804\n",
            "133 0.46734846\n",
            "134 0.44000253\n",
            "135 0.36752456\n",
            "136 0.41975638\n",
            "137 0.3747963\n",
            "138 0.54633373\n",
            "139 0.47431946\n",
            "140 0.27354118\n",
            "141 0.3288276\n",
            "142 0.27421784\n",
            "143 0.626956\n",
            "144 0.38892743\n",
            "145 0.2236076\n",
            "146 0.36853814\n",
            "147 0.6285537\n",
            "148 0.4308669\n",
            "149 0.34245637\n",
            "150 0.31197232\n",
            "151 0.5095547\n",
            "152 0.51087457\n",
            "153 0.31679776\n",
            "154 0.24224724\n",
            "155 0.2607617\n",
            "156 0.44050124\n",
            "157 0.3497252\n",
            "158 0.40354875\n",
            "159 0.36148238\n",
            "160 0.14112766\n",
            "161 0.34945068\n",
            "162 0.3180904\n",
            "163 0.35414338\n",
            "164 0.39698592\n",
            "165 0.22435208\n",
            "166 0.34218657\n",
            "167 0.29114518\n",
            "168 0.1906532\n",
            "169 0.48736665\n",
            "170 0.27499175\n",
            "171 0.2729521\n",
            "172 0.39271665\n",
            "173 0.45808765\n",
            "174 0.37780812\n",
            "175 0.3290167\n",
            "176 0.22478777\n",
            "177 0.34913117\n",
            "178 0.21320005\n",
            "179 0.27887455\n",
            "180 0.5016475\n",
            "181 0.4104166\n",
            "182 0.3800697\n",
            "183 0.2971088\n",
            "184 0.4170231\n",
            "185 0.20228027\n",
            "186 0.4052557\n",
            "187 0.3688425\n",
            "188 0.36788857\n",
            "189 0.24698883\n",
            "190 0.22217916\n",
            "191 0.24323148\n",
            "192 0.2912091\n",
            "193 0.26074612\n",
            "194 0.3814962\n",
            "195 0.3419114\n",
            "196 0.41097978\n",
            "197 0.43933573\n",
            "198 0.43412384\n",
            "199 0.22129957\n",
            "200 0.27499115\n",
            "201 0.38133395\n",
            "202 0.33023566\n",
            "203 0.20676851\n",
            "204 0.26427007\n",
            "205 0.34253624\n",
            "206 0.2625363\n",
            "207 0.34574383\n",
            "208 0.24904056\n",
            "209 0.18969317\n",
            "210 0.42807594\n",
            "211 0.311456\n",
            "212 0.41475117\n",
            "213 0.25331688\n",
            "214 0.21643794\n",
            "215 0.20258245\n",
            "216 0.15826777\n",
            "217 0.49056444\n",
            "218 0.24562977\n",
            "219 0.35389817\n",
            "220 0.20050758\n",
            "221 0.26729113\n",
            "222 0.34604245\n",
            "223 0.6394161\n",
            "224 0.42530745\n",
            "225 0.38817272\n",
            "226 0.24335861\n",
            "227 0.46543396\n",
            "228 0.16309804\n",
            "229 0.16583042\n",
            "230 0.41999313\n",
            "231 0.30154067\n",
            "232 0.42694286\n",
            "233 0.21901394\n",
            "234 0.4439217\n",
            "235 0.51559764\n",
            "236 0.3843062\n",
            "237 0.3950375\n",
            "238 0.36763322\n",
            "239 0.2849143\n",
            "240 0.2618702\n",
            "241 0.2601328\n",
            "242 0.33631784\n",
            "243 0.40072\n",
            "244 0.17552976\n",
            "245 0.25199232\n",
            "246 0.31996202\n",
            "247 0.45103672\n",
            "248 0.32680827\n",
            "249 0.19740559\n",
            "250 0.36028063\n",
            "251 0.21322101\n",
            "252 0.39892718\n",
            "253 0.3552795\n",
            "254 0.2572391\n",
            "255 0.23529343\n",
            "256 0.25473624\n",
            "257 0.27678934\n",
            "258 0.30417737\n",
            "259 0.4560148\n",
            "260 0.34857738\n",
            "261 0.21980852\n",
            "262 0.34721452\n",
            "263 0.4423715\n",
            "264 0.12569627\n",
            "265 0.42278472\n",
            "266 0.29927334\n",
            "267 0.4914119\n",
            "268 0.55853945\n",
            "269 0.21779771\n",
            "270 0.23450497\n",
            "271 0.5335233\n",
            "272 0.42656082\n",
            "273 0.22043037\n",
            "274 0.25360203\n",
            "275 0.34508073\n",
            "276 0.34900162\n",
            "277 0.35242325\n",
            "278 0.37215024\n",
            "279 0.14893635\n",
            "280 0.26254857\n",
            "281 0.3127452\n",
            "282 0.18532187\n",
            "283 0.6161905\n",
            "284 0.31246686\n",
            "285 0.21390723\n",
            "286 0.22163993\n",
            "287 0.36025307\n",
            "288 0.18491508\n",
            "289 0.25870135\n",
            "290 0.18040018\n",
            "291 0.29519314\n",
            "292 0.23536228\n",
            "293 0.29364058\n",
            "294 0.3378249\n",
            "295 0.33719483\n",
            "296 0.3026358\n",
            "297 0.3591885\n",
            "298 0.36544842\n",
            "299 0.22883429\n",
            "300 0.14802983\n",
            "301 0.40245563\n",
            "302 0.3196109\n",
            "303 0.5108718\n",
            "304 0.672498\n",
            "305 0.40751943\n",
            "306 0.3325609\n",
            "307 0.42000553\n",
            "308 0.27107012\n",
            "309 0.32501906\n",
            "310 0.2986315\n",
            "311 0.21680123\n",
            "312 0.34553722\n",
            "313 0.38324043\n",
            "314 0.18876381\n",
            "315 0.45739815\n",
            "316 0.17593548\n",
            "317 0.28758645\n",
            "318 0.4062786\n",
            "319 0.45071998\n",
            "320 0.25119874\n",
            "321 0.4212355\n",
            "322 0.34811568\n",
            "323 0.27287412\n",
            "324 0.29528764\n",
            "325 0.30481172\n",
            "326 0.15747331\n",
            "327 0.12957686\n",
            "328 0.083952256\n",
            "329 0.25068152\n",
            "330 0.28897744\n",
            "331 0.31544867\n",
            "332 0.41755393\n",
            "333 0.4233922\n",
            "334 0.43728638\n",
            "335 0.4687887\n",
            "336 0.3526583\n",
            "337 0.20080811\n",
            "338 0.5955648\n",
            "339 0.3491908\n",
            "340 0.3390094\n",
            "341 0.33239746\n",
            "342 0.28594708\n",
            "343 0.2545306\n",
            "344 0.24139403\n",
            "345 0.25358504\n",
            "346 0.3353301\n",
            "347 0.19788338\n",
            "348 0.105133064\n",
            "349 0.24995965\n",
            "350 0.4707093\n",
            "351 0.123404056\n",
            "352 0.15343857\n",
            "353 0.15356821\n",
            "354 0.15662873\n",
            "355 0.7970512\n",
            "356 0.15847391\n",
            "357 0.31153932\n",
            "358 0.3703312\n",
            "359 0.1921884\n",
            "360 0.13936184\n",
            "361 0.23943576\n",
            "362 0.14699914\n",
            "363 0.3652439\n",
            "364 0.39083374\n",
            "365 0.21160366\n",
            "366 0.27341878\n",
            "367 0.29853344\n",
            "368 0.23170465\n",
            "369 0.2950263\n",
            "370 0.29745215\n",
            "371 0.11629555\n",
            "372 0.38794434\n",
            "373 0.2151174\n",
            "374 0.3094817\n",
            "375 0.30499986\n",
            "376 0.07253399\n",
            "377 0.18408546\n",
            "378 0.5549118\n",
            "379 0.18549608\n",
            "380 0.33746043\n",
            "381 0.20531504\n",
            "382 0.47854817\n",
            "383 0.20194557\n",
            "384 0.53783745\n",
            "385 0.36827815\n",
            "386 0.31585968\n",
            "387 0.3140178\n",
            "388 0.18406814\n",
            "389 0.32249096\n",
            "390 0.164893\n",
            "391 0.26017398\n",
            "392 0.28759414\n",
            "393 0.41506806\n",
            "394 0.12886195\n",
            "395 0.2181414\n",
            "396 0.25729856\n",
            "397 0.16965663\n",
            "398 0.1765982\n",
            "399 0.17847171\n",
            "400 0.3186244\n",
            "401 0.32709488\n",
            "402 0.27759078\n",
            "403 0.15350023\n",
            "404 0.3607675\n",
            "405 0.5619614\n",
            "406 0.27295434\n",
            "407 0.22123909\n",
            "408 0.17771994\n",
            "409 0.14054573\n",
            "410 0.38234437\n",
            "411 0.41786247\n",
            "412 0.2772528\n",
            "413 0.25554198\n",
            "414 0.62686515\n",
            "415 0.23114845\n",
            "416 0.45659503\n",
            "417 0.19910565\n",
            "418 0.2125778\n",
            "419 0.22334503\n",
            "420 0.21816337\n",
            "421 0.3683261\n",
            "422 0.33533183\n",
            "423 0.2360183\n",
            "424 0.20273967\n",
            "425 0.22738375\n",
            "426 0.3782586\n",
            "427 0.15257242\n",
            "428 0.12959383\n",
            "429 0.11698948\n",
            "430 0.14304441\n",
            "431 0.31323367\n",
            "432 0.16056737\n",
            "433 0.6023298\n",
            "434 0.22126575\n",
            "435 0.49490124\n",
            "436 0.19753517\n",
            "437 0.32543945\n",
            "438 0.30061343\n",
            "439 0.27243555\n",
            "440 0.28751317\n",
            "441 0.44593468\n",
            "442 0.39311114\n",
            "443 0.36936444\n",
            "444 0.34740242\n",
            "445 0.20235077\n",
            "446 0.19067995\n",
            "447 0.34471408\n",
            "448 0.35912347\n",
            "449 0.45920956\n",
            "450 0.40531594\n",
            "451 0.17951801\n",
            "452 0.34947658\n",
            "453 0.59759444\n",
            "454 0.2001221\n",
            "455 0.24722712\n",
            "456 0.28147182\n",
            "457 0.26811543\n",
            "458 0.119429894\n",
            "459 0.28686664\n",
            "460 0.24866384\n",
            "461 0.22341706\n",
            "462 0.24942487\n",
            "463 0.20126957\n",
            "464 0.38978207\n",
            "465 0.16316348\n",
            "466 0.23350418\n",
            "467 0.20498437\n",
            "468 0.23326674\n",
            "469 0.20694183\n",
            "470 0.3640724\n",
            "471 0.20264824\n",
            "472 0.3770781\n",
            "473 0.20267595\n",
            "474 0.18932642\n",
            "475 0.20274028\n",
            "476 0.111824684\n",
            "477 0.3235726\n",
            "478 0.20762496\n",
            "479 0.14915563\n",
            "480 0.16020381\n",
            "481 0.09829048\n",
            "482 0.18668468\n",
            "483 0.29878053\n",
            "484 0.36157268\n",
            "485 0.13452303\n",
            "486 0.2601909\n",
            "487 0.16995144\n",
            "488 0.3627426\n",
            "489 0.15444288\n",
            "490 0.17251278\n",
            "491 0.27629024\n",
            "492 0.19443005\n",
            "493 0.20914766\n",
            "494 0.24978755\n",
            "495 0.18276931\n",
            "496 0.19139569\n",
            "497 0.29052356\n",
            "498 0.20298636\n",
            "499 0.18015085\n",
            "500 0.11181408\n",
            "501 0.2625792\n",
            "502 0.25754988\n",
            "503 0.14331399\n",
            "504 0.1886084\n",
            "505 0.16750363\n",
            "506 0.22457613\n",
            "507 0.4277692\n",
            "508 0.1281993\n",
            "509 0.19045982\n",
            "510 0.11724077\n",
            "511 0.12073204\n",
            "512 0.34574518\n",
            "513 0.3420459\n",
            "514 0.26643097\n",
            "515 0.24118298\n",
            "516 0.20978272\n",
            "517 0.2113711\n",
            "518 0.30592534\n",
            "519 0.24030747\n",
            "520 0.12421564\n",
            "521 0.17285499\n",
            "522 0.14348316\n",
            "523 0.21713428\n",
            "524 0.15626267\n",
            "525 0.185881\n",
            "526 0.3680771\n",
            "527 0.24386093\n",
            "528 0.39735043\n",
            "529 0.13618359\n",
            "530 0.31375343\n",
            "531 0.29842392\n",
            "532 0.3051772\n",
            "533 0.24676283\n",
            "534 0.35436696\n",
            "535 0.19337656\n",
            "536 0.22875422\n",
            "537 0.24579369\n",
            "538 0.52202207\n",
            "539 0.18238068\n",
            "540 0.20388551\n",
            "541 0.12293141\n",
            "542 0.22061695\n",
            "543 0.16580296\n",
            "544 0.21994196\n",
            "545 0.39406118\n",
            "546 0.3382943\n",
            "547 0.26893994\n",
            "548 0.15888989\n",
            "549 0.3271435\n",
            "550 0.20055792\n",
            "551 0.17848474\n",
            "552 0.32469785\n",
            "553 0.19151972\n",
            "554 0.17764404\n",
            "555 0.10220711\n",
            "556 0.12450941\n",
            "557 0.16797005\n",
            "558 0.26322392\n",
            "559 0.20652759\n",
            "560 0.16220632\n",
            "561 0.22622456\n",
            "562 0.42422324\n",
            "563 0.20620967\n",
            "564 0.10584244\n",
            "565 0.24176992\n",
            "566 0.06452491\n",
            "567 0.08867287\n",
            "568 0.44258612\n",
            "569 0.25874135\n",
            "570 0.25012693\n",
            "571 0.1039128\n",
            "572 0.28508428\n",
            "573 0.47294867\n",
            "574 0.4035\n",
            "575 0.22910899\n",
            "576 0.24312007\n",
            "577 0.28775012\n",
            "578 0.29763782\n",
            "579 0.27652997\n",
            "580 0.20730847\n",
            "581 0.39099246\n",
            "582 0.31505936\n",
            "583 0.16520172\n",
            "584 0.15022157\n",
            "585 0.31151766\n",
            "586 0.16709642\n",
            "587 0.25431785\n",
            "588 0.23497576\n",
            "589 0.21311554\n",
            "590 0.15145916\n",
            "591 0.2193199\n",
            "592 0.23345968\n",
            "593 0.2748737\n",
            "594 0.16002525\n",
            "595 0.216261\n",
            "596 0.21347305\n",
            "597 0.27532718\n",
            "598 0.37155738\n",
            "599 0.24329701\n",
            "600 0.06994596\n",
            "601 0.29606593\n",
            "602 0.23220474\n",
            "603 0.09435849\n",
            "604 0.36886033\n",
            "605 0.28285235\n",
            "606 0.20956512\n",
            "607 0.23328313\n",
            "608 0.3786911\n",
            "609 0.25163078\n",
            "610 0.2993209\n",
            "611 0.28064433\n",
            "612 0.28830382\n",
            "613 0.23018229\n",
            "614 0.15540384\n",
            "615 0.30271158\n",
            "616 0.05891197\n",
            "617 0.12298186\n",
            "618 0.100441016\n",
            "619 0.59372866\n",
            "620 0.27249753\n",
            "621 0.16906549\n",
            "622 0.23737845\n",
            "623 0.10509926\n",
            "624 0.22472471\n",
            "625 0.24583744\n",
            "626 0.16215183\n",
            "627 0.21839069\n",
            "628 0.46205702\n",
            "629 0.25653297\n",
            "630 0.3043228\n",
            "631 0.14083019\n",
            "632 0.2500718\n",
            "633 0.17499954\n",
            "634 0.16741276\n",
            "635 0.25565526\n",
            "636 0.26594412\n",
            "637 0.2536712\n",
            "638 0.30100957\n",
            "639 0.15790638\n",
            "640 0.2496938\n",
            "641 0.3419713\n",
            "642 0.15394911\n",
            "643 0.10942476\n",
            "644 0.16153507\n",
            "645 0.2624536\n",
            "646 0.111733876\n",
            "647 0.32863548\n",
            "648 0.2844492\n",
            "649 0.28668243\n",
            "650 0.07939953\n",
            "651 0.3683748\n",
            "652 0.26791272\n",
            "653 0.2395499\n",
            "654 0.33403945\n",
            "655 0.24224907\n",
            "656 0.22304295\n",
            "657 0.16250986\n",
            "658 0.17491281\n",
            "659 0.2989809\n",
            "660 0.37715992\n",
            "661 0.6172712\n",
            "662 0.40918934\n",
            "663 0.21398437\n",
            "664 0.24478592\n",
            "665 0.47149605\n",
            "666 0.1729166\n",
            "667 0.1524609\n",
            "668 0.22866455\n",
            "669 0.16900878\n",
            "670 0.1164563\n",
            "671 0.18664032\n",
            "672 0.16373082\n",
            "673 0.22989716\n",
            "674 0.118656605\n",
            "675 0.21116284\n",
            "676 0.15124911\n",
            "677 0.22165374\n",
            "678 0.23750031\n",
            "679 0.22090244\n",
            "680 0.16338544\n",
            "681 0.12093702\n",
            "682 0.26070476\n",
            "683 0.12032137\n",
            "684 0.1548777\n",
            "685 0.16586411\n",
            "686 0.24425879\n",
            "687 0.26733702\n",
            "688 0.1590748\n",
            "689 0.394515\n",
            "690 0.10255398\n",
            "691 0.30354267\n",
            "692 0.09259424\n",
            "693 0.16570204\n",
            "694 0.13027868\n",
            "695 0.31482914\n",
            "696 0.28554037\n",
            "697 0.3816944\n",
            "698 0.1136174\n",
            "699 0.2039571\n",
            "700 0.19521795\n",
            "701 0.17043093\n",
            "702 0.2121201\n",
            "703 0.1596015\n",
            "704 0.09765975\n",
            "705 0.3320565\n",
            "706 0.22028333\n",
            "707 0.21413292\n",
            "708 0.2381287\n",
            "709 0.3462579\n",
            "710 0.14291836\n",
            "711 0.1537921\n",
            "712 0.08489303\n",
            "713 0.30689022\n",
            "714 0.12566243\n",
            "715 0.29410577\n",
            "716 0.25747284\n",
            "717 0.34677193\n",
            "718 0.2818953\n",
            "719 0.13134408\n",
            "720 0.33188817\n",
            "721 0.093859844\n",
            "722 0.17577991\n",
            "723 0.2823848\n",
            "724 0.11789915\n",
            "725 0.0712985\n",
            "726 0.25766918\n",
            "727 0.16433354\n",
            "728 0.15677133\n",
            "729 0.25339893\n",
            "730 0.49661785\n",
            "731 0.22129379\n",
            "732 0.14722775\n",
            "733 0.19302414\n",
            "734 0.273884\n",
            "735 0.32122064\n",
            "736 0.1355403\n",
            "737 0.1378285\n",
            "738 0.14149204\n",
            "739 0.3360666\n",
            "740 0.21356316\n",
            "741 0.3080124\n",
            "742 0.24617504\n",
            "743 0.17291489\n",
            "744 0.24417892\n",
            "745 0.17477585\n",
            "746 0.15651909\n",
            "747 0.25027502\n",
            "748 0.20507795\n",
            "749 0.21277311\n",
            "750 0.171172\n",
            "751 0.11915994\n",
            "752 0.08669861\n",
            "753 0.39180526\n",
            "754 0.1939732\n",
            "755 0.4251918\n",
            "756 0.27785406\n",
            "757 0.25668675\n",
            "758 0.13493487\n",
            "759 0.3913784\n",
            "760 0.33107585\n",
            "761 0.37566483\n",
            "762 0.2625891\n",
            "763 0.33158243\n",
            "764 0.17280644\n",
            "765 0.21778467\n",
            "766 0.15370184\n",
            "767 0.11655367\n",
            "768 0.21033098\n",
            "769 0.11022945\n",
            "770 0.36918136\n",
            "771 0.14189078\n",
            "772 0.13114059\n",
            "773 0.38599968\n",
            "774 0.26121497\n",
            "775 0.32963553\n",
            "776 0.102413476\n",
            "777 0.20163582\n",
            "778 0.18689543\n",
            "779 0.21807629\n",
            "780 0.17862724\n",
            "781 0.092392616\n",
            "782 0.39218643\n",
            "783 0.21777192\n",
            "784 0.1482593\n",
            "785 0.15864618\n",
            "786 0.26682448\n",
            "787 0.25394154\n",
            "788 0.08585329\n",
            "789 0.13724828\n",
            "790 0.2694465\n",
            "791 0.28792575\n",
            "792 0.14140506\n",
            "793 0.17435859\n",
            "794 0.12773955\n",
            "795 0.29600835\n",
            "796 0.15257442\n",
            "797 0.22602867\n",
            "798 0.15888283\n",
            "799 0.08016759\n",
            "800 0.3465582\n",
            "801 0.13928828\n",
            "802 0.23213191\n",
            "803 0.11063721\n",
            "804 0.14501843\n",
            "805 0.18158138\n",
            "806 0.4560183\n",
            "807 0.41807076\n",
            "808 0.091930084\n",
            "809 0.11095625\n",
            "810 0.12583548\n",
            "811 0.104888156\n",
            "812 0.37459725\n",
            "813 0.37425712\n",
            "814 0.2767139\n",
            "815 0.049414188\n",
            "816 0.15450402\n",
            "817 0.33103982\n",
            "818 0.14258641\n",
            "819 0.2074096\n",
            "820 0.12774558\n",
            "821 0.15844296\n",
            "822 0.2583968\n",
            "823 0.4367237\n",
            "824 0.16251816\n",
            "825 0.13586777\n",
            "826 0.07657466\n",
            "827 0.14481708\n",
            "828 0.11772838\n",
            "829 0.2542366\n",
            "830 0.06169767\n",
            "831 0.22539645\n",
            "832 0.11102871\n",
            "833 0.1440198\n",
            "834 0.2646696\n",
            "835 0.11390273\n",
            "836 0.21635936\n",
            "837 0.18751758\n",
            "838 0.2807145\n",
            "839 0.15005894\n",
            "840 0.2518016\n",
            "841 0.1685971\n",
            "842 0.11074001\n",
            "843 0.0696982\n",
            "844 0.10092442\n",
            "845 0.2816383\n",
            "846 0.33194855\n",
            "847 0.12950245\n",
            "848 0.084304236\n",
            "849 0.18613724\n",
            "850 0.12748276\n",
            "851 0.25382328\n",
            "852 0.08608889\n",
            "853 0.111107804\n",
            "854 0.19465809\n",
            "855 0.069235735\n",
            "856 0.1361154\n",
            "857 0.25953612\n",
            "858 0.12610014\n",
            "859 0.21644682\n",
            "860 0.12511954\n",
            "861 0.1217902\n",
            "862 0.21626505\n",
            "863 0.229701\n",
            "864 0.17504376\n",
            "865 0.16404705\n",
            "866 0.21351342\n",
            "867 0.109417535\n",
            "868 0.15937117\n",
            "869 0.10046198\n",
            "870 0.13875003\n",
            "871 0.14224133\n",
            "872 0.23976855\n",
            "873 0.122980654\n",
            "874 0.11717306\n",
            "875 0.114338115\n",
            "876 0.11396387\n",
            "877 0.10968792\n",
            "878 0.10435954\n",
            "879 0.29440325\n",
            "880 0.31991425\n",
            "881 0.10933212\n",
            "882 0.17265728\n",
            "883 0.11040302\n",
            "884 0.13840418\n",
            "885 0.16634344\n",
            "886 0.15360671\n",
            "887 0.08271051\n",
            "888 0.26998532\n",
            "889 0.08819951\n",
            "890 0.07136179\n",
            "891 0.29172024\n",
            "892 0.21677917\n",
            "893 0.15661395\n",
            "894 0.14575464\n",
            "895 0.12957866\n",
            "896 0.14314456\n",
            "897 0.21730629\n",
            "898 0.20283617\n",
            "899 0.20550708\n",
            "900 0.13537781\n",
            "901 0.09618831\n",
            "902 0.30826986\n",
            "903 0.123044394\n",
            "904 0.12230242\n",
            "905 0.3522254\n",
            "906 0.10496671\n",
            "907 0.15428977\n",
            "908 0.089018725\n",
            "909 0.1560538\n",
            "910 0.1934666\n",
            "911 0.17600209\n",
            "912 0.09373381\n",
            "913 0.1304829\n",
            "914 0.18638691\n",
            "915 0.17073388\n",
            "916 0.6340593\n",
            "917 0.41487083\n",
            "918 0.20531158\n",
            "919 0.14011875\n",
            "920 0.25948787\n",
            "921 0.17854847\n",
            "922 0.14182022\n",
            "923 0.18714584\n",
            "924 0.41103283\n",
            "925 0.10312347\n",
            "926 0.1920685\n",
            "927 0.20402154\n",
            "928 0.039632272\n",
            "929 0.075946264\n",
            "930 0.13935375\n",
            "931 0.3900847\n",
            "932 0.13474222\n",
            "933 0.096098006\n",
            "934 0.08818048\n",
            "935 0.20428841\n",
            "936 0.3859343\n",
            "937 0.22671677\n",
            "938 0.21753475\n",
            "939 0.24302465\n",
            "940 0.13750075\n",
            "941 0.16784966\n",
            "942 0.11913512\n",
            "943 0.18535991\n",
            "944 0.147005\n",
            "945 0.088631555\n",
            "946 0.23694862\n",
            "947 0.18935621\n",
            "948 0.30782336\n",
            "949 0.04527165\n",
            "950 0.082934916\n",
            "951 0.33070323\n",
            "952 0.15559722\n",
            "953 0.14571382\n",
            "954 0.30519217\n",
            "955 0.21955004\n",
            "956 0.15733704\n",
            "957 0.31450337\n",
            "958 0.1249188\n",
            "959 0.11278043\n",
            "960 0.22619395\n",
            "961 0.09665652\n",
            "962 0.150061\n",
            "963 0.2161101\n",
            "964 0.09008889\n",
            "965 0.31046572\n",
            "966 0.26438558\n",
            "967 0.07502705\n",
            "968 0.25200424\n",
            "969 0.11464823\n",
            "970 0.088598415\n",
            "971 0.18359986\n",
            "972 0.22085033\n",
            "973 0.108440705\n",
            "974 0.14430796\n",
            "975 0.1981666\n",
            "976 0.2827255\n",
            "977 0.1042791\n",
            "978 0.07838543\n",
            "979 0.10362093\n",
            "980 0.041641522\n",
            "981 0.072067715\n",
            "982 0.13148238\n",
            "983 0.14277434\n",
            "984 0.17553307\n",
            "985 0.07384422\n",
            "986 0.077412754\n",
            "987 0.3272668\n",
            "988 0.13397722\n",
            "989 0.3108238\n",
            "990 0.3104628\n",
            "991 0.08322845\n",
            "992 0.11137877\n",
            "993 0.127426\n",
            "994 0.15772691\n",
            "995 0.2234156\n",
            "996 0.087546445\n",
            "997 0.1777091\n",
            "998 0.3193761\n",
            "999 0.10236113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fl6YFNCziovW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "470afb4e-5b7c-4b39-ed3d-dcbf4ccc8221"
      },
      "cell_type": "code",
      "source": [
        "num_eval_samples = np.shape(data_loader.eval_labels)[0]\n",
        "\n",
        "y_pred = model.predict(data_loader.eval_data).numpy() \n",
        "\n",
        "print(\"accuracy:%f\"%(sum(y_pred == data_loader.eval_labels) / num_eval_samples))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy:0.948800\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}