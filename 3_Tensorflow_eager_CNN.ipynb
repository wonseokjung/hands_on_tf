{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Tensorflow_eager_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/wonseokjung/hands_on_tf/blob/master/3_Tensorflow_eager_CNN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Zu6KIb552LP-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGAhAAep2g2s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution() \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4Ab49uu2qik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1= tf.keras.layers.Conv2D(\n",
        "      filters=32, \n",
        "      kernel_size = [5,5],\n",
        "      padding=\"same\",\n",
        "      activation = tf.nn.relu \n",
        "    )\n",
        "    \n",
        "    self.pool1 = tf.keras.layers.MaxPool2D(pool_size =[2,2], strides = 2)\n",
        "    self.conv2=tf.keras.layers.Conv2D(filters = 64,\n",
        "                                      kernel_size = [5,5],\n",
        "                                      padding =\"same\",\n",
        "                                      activation = tf.nn.relu\n",
        "                                     )\n",
        "    self.pool2 = tf.keras.layers.MaxPool2D(pool_size = [2,2], strides = 2)\n",
        "    \n",
        "    self.flatten = tf.keras.layers.Reshape(target_shape = (7*7*64,))\n",
        "    \n",
        "    self.dense1 = tf.keras.layers.Dense(units = 1024, activation = tf.nn.relu)\n",
        "   \n",
        "    \n",
        "    self.dense2 = tf.keras.layers.Dense(units = 10)\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "  def call(self, inputs): \n",
        "    inputs = tf.reshape(inputs, [-1,28,28,1])\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool2(x)\n",
        "    x= self.flatten(x)\n",
        "    x=self.dense1(x)\n",
        "    x=self.dense2(x)\n",
        "    return x\n",
        "   \n",
        "    \n",
        "  def predict(self, inputs):\n",
        "    logits = self(inputs)\n",
        "    return tf.argmax(logits, axis = 1 )\n",
        "  \n",
        " \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vg0iu8oO2qlc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataLoader():\n",
        "  def __init__(self):\n",
        "    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "    \n",
        "    self.train_data = mnist.train.images \n",
        "    self.train_labels = np.asarray(mnist.train.labels, dtype= np.int32)\n",
        "    self.eval_data = mnist.test.images \n",
        "    self.eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
        "   \n",
        "\n",
        "  def get_batch(self, batch_size):\n",
        "    index = np.random.randint(0,np.shape(self.train_data)[0], batch_size)\n",
        "    \n",
        "    return self.train_data[index, :], self.train_labels[index]\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnY16xWyAnp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "dea50e66-6b2c-4a47-e896-5d43d00b3284"
      },
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "data_loader = DataLoader()\n",
        "\n",
        "num_batches = 1000\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-24bec8f9d971>:3: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zNOSsqUk2qoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17017
        },
        "outputId": "ef8bb46d-e3cf-451b-9977-3c0915b56022"
      },
      "cell_type": "code",
      "source": [
        "for batch_index in range(num_batches):\n",
        "    X, y = data_loader.get_batch(batch_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_logit_pred = model(tf.convert_to_tensor(X))\n",
        "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)\n",
        "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
        "    grads = tape.gradient(loss, model.variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0: loss 2.289092\n",
            "batch 1: loss 2.227195\n",
            "batch 2: loss 2.087518\n",
            "batch 3: loss 1.881793\n",
            "batch 4: loss 1.714403\n",
            "batch 5: loss 1.611320\n",
            "batch 6: loss 1.365711\n",
            "batch 7: loss 0.936148\n",
            "batch 8: loss 0.958978\n",
            "batch 9: loss 0.966190\n",
            "batch 10: loss 0.739592\n",
            "batch 11: loss 0.737145\n",
            "batch 12: loss 0.731559\n",
            "batch 13: loss 0.555802\n",
            "batch 14: loss 0.714620\n",
            "batch 15: loss 0.539785\n",
            "batch 16: loss 0.366553\n",
            "batch 17: loss 0.734777\n",
            "batch 18: loss 0.591951\n",
            "batch 19: loss 0.348320\n",
            "batch 20: loss 0.462288\n",
            "batch 21: loss 0.424636\n",
            "batch 22: loss 0.467608\n",
            "batch 23: loss 0.439074\n",
            "batch 24: loss 0.361790\n",
            "batch 25: loss 0.744539\n",
            "batch 26: loss 0.331172\n",
            "batch 27: loss 0.230847\n",
            "batch 28: loss 0.348492\n",
            "batch 29: loss 0.249755\n",
            "batch 30: loss 0.545975\n",
            "batch 31: loss 0.420389\n",
            "batch 32: loss 0.522436\n",
            "batch 33: loss 0.511936\n",
            "batch 34: loss 0.398413\n",
            "batch 35: loss 0.352756\n",
            "batch 36: loss 0.272183\n",
            "batch 37: loss 0.265751\n",
            "batch 38: loss 0.272731\n",
            "batch 39: loss 0.365220\n",
            "batch 40: loss 0.439644\n",
            "batch 41: loss 0.118752\n",
            "batch 42: loss 0.115451\n",
            "batch 43: loss 0.281202\n",
            "batch 44: loss 0.217501\n",
            "batch 45: loss 0.105833\n",
            "batch 46: loss 0.254123\n",
            "batch 47: loss 0.159982\n",
            "batch 48: loss 0.306396\n",
            "batch 49: loss 0.098628\n",
            "batch 50: loss 0.373802\n",
            "batch 51: loss 0.604404\n",
            "batch 52: loss 0.246879\n",
            "batch 53: loss 0.206010\n",
            "batch 54: loss 0.049547\n",
            "batch 55: loss 0.209169\n",
            "batch 56: loss 0.186010\n",
            "batch 57: loss 0.409128\n",
            "batch 58: loss 0.101899\n",
            "batch 59: loss 0.110293\n",
            "batch 60: loss 0.077787\n",
            "batch 61: loss 0.277004\n",
            "batch 62: loss 0.105285\n",
            "batch 63: loss 0.279641\n",
            "batch 64: loss 0.345956\n",
            "batch 65: loss 0.049946\n",
            "batch 66: loss 0.122777\n",
            "batch 67: loss 0.233121\n",
            "batch 68: loss 0.075090\n",
            "batch 69: loss 0.176692\n",
            "batch 70: loss 0.311925\n",
            "batch 71: loss 0.087530\n",
            "batch 72: loss 0.096381\n",
            "batch 73: loss 0.143218\n",
            "batch 74: loss 0.136703\n",
            "batch 75: loss 0.147660\n",
            "batch 76: loss 0.125171\n",
            "batch 77: loss 0.343453\n",
            "batch 78: loss 0.383702\n",
            "batch 79: loss 0.405836\n",
            "batch 80: loss 0.132023\n",
            "batch 81: loss 0.303018\n",
            "batch 82: loss 0.021910\n",
            "batch 83: loss 0.092230\n",
            "batch 84: loss 0.388084\n",
            "batch 85: loss 0.143693\n",
            "batch 86: loss 0.095192\n",
            "batch 87: loss 0.110127\n",
            "batch 88: loss 0.151353\n",
            "batch 89: loss 0.087925\n",
            "batch 90: loss 0.164965\n",
            "batch 91: loss 0.240963\n",
            "batch 92: loss 0.223971\n",
            "batch 93: loss 0.106092\n",
            "batch 94: loss 0.250732\n",
            "batch 95: loss 0.177449\n",
            "batch 96: loss 0.132924\n",
            "batch 97: loss 0.361549\n",
            "batch 98: loss 0.211126\n",
            "batch 99: loss 0.211354\n",
            "batch 100: loss 0.075908\n",
            "batch 101: loss 0.050719\n",
            "batch 102: loss 0.159749\n",
            "batch 103: loss 0.268091\n",
            "batch 104: loss 0.114357\n",
            "batch 105: loss 0.097238\n",
            "batch 106: loss 0.140134\n",
            "batch 107: loss 0.105211\n",
            "batch 108: loss 0.153370\n",
            "batch 109: loss 0.184538\n",
            "batch 110: loss 0.060168\n",
            "batch 111: loss 0.252110\n",
            "batch 112: loss 0.203240\n",
            "batch 113: loss 0.283397\n",
            "batch 114: loss 0.151304\n",
            "batch 115: loss 0.284503\n",
            "batch 116: loss 0.129598\n",
            "batch 117: loss 0.116028\n",
            "batch 118: loss 0.200154\n",
            "batch 119: loss 0.127706\n",
            "batch 120: loss 0.380850\n",
            "batch 121: loss 0.231547\n",
            "batch 122: loss 0.083932\n",
            "batch 123: loss 0.210465\n",
            "batch 124: loss 0.171990\n",
            "batch 125: loss 0.158317\n",
            "batch 126: loss 0.171353\n",
            "batch 127: loss 0.111114\n",
            "batch 128: loss 0.118377\n",
            "batch 129: loss 0.040388\n",
            "batch 130: loss 0.196200\n",
            "batch 131: loss 0.088517\n",
            "batch 132: loss 0.051456\n",
            "batch 133: loss 0.091430\n",
            "batch 134: loss 0.092790\n",
            "batch 135: loss 0.224209\n",
            "batch 136: loss 0.122968\n",
            "batch 137: loss 0.212015\n",
            "batch 138: loss 0.133146\n",
            "batch 139: loss 0.089963\n",
            "batch 140: loss 0.050910\n",
            "batch 141: loss 0.232671\n",
            "batch 142: loss 0.062824\n",
            "batch 143: loss 0.124455\n",
            "batch 144: loss 0.197893\n",
            "batch 145: loss 0.141117\n",
            "batch 146: loss 0.133667\n",
            "batch 147: loss 0.077001\n",
            "batch 148: loss 0.198617\n",
            "batch 149: loss 0.320272\n",
            "batch 150: loss 0.147317\n",
            "batch 151: loss 0.486440\n",
            "batch 152: loss 0.212812\n",
            "batch 153: loss 0.032719\n",
            "batch 154: loss 0.076097\n",
            "batch 155: loss 0.194471\n",
            "batch 156: loss 0.119834\n",
            "batch 157: loss 0.141236\n",
            "batch 158: loss 0.039434\n",
            "batch 159: loss 0.126824\n",
            "batch 160: loss 0.388274\n",
            "batch 161: loss 0.138783\n",
            "batch 162: loss 0.044931\n",
            "batch 163: loss 0.119278\n",
            "batch 164: loss 0.138440\n",
            "batch 165: loss 0.155642\n",
            "batch 166: loss 0.302454\n",
            "batch 167: loss 0.076050\n",
            "batch 168: loss 0.331866\n",
            "batch 169: loss 0.187962\n",
            "batch 170: loss 0.091381\n",
            "batch 171: loss 0.337371\n",
            "batch 172: loss 0.056548\n",
            "batch 173: loss 0.152588\n",
            "batch 174: loss 0.135703\n",
            "batch 175: loss 0.130674\n",
            "batch 176: loss 0.110507\n",
            "batch 177: loss 0.165441\n",
            "batch 178: loss 0.141067\n",
            "batch 179: loss 0.263553\n",
            "batch 180: loss 0.140826\n",
            "batch 181: loss 0.099571\n",
            "batch 182: loss 0.009933\n",
            "batch 183: loss 0.151223\n",
            "batch 184: loss 0.165229\n",
            "batch 185: loss 0.050336\n",
            "batch 186: loss 0.290604\n",
            "batch 187: loss 0.046046\n",
            "batch 188: loss 0.058604\n",
            "batch 189: loss 0.093973\n",
            "batch 190: loss 0.037175\n",
            "batch 191: loss 0.246362\n",
            "batch 192: loss 0.085976\n",
            "batch 193: loss 0.044550\n",
            "batch 194: loss 0.132229\n",
            "batch 195: loss 0.122231\n",
            "batch 196: loss 0.097503\n",
            "batch 197: loss 0.305423\n",
            "batch 198: loss 0.070112\n",
            "batch 199: loss 0.081715\n",
            "batch 200: loss 0.137476\n",
            "batch 201: loss 0.106703\n",
            "batch 202: loss 0.034054\n",
            "batch 203: loss 0.041153\n",
            "batch 204: loss 0.079076\n",
            "batch 205: loss 0.091143\n",
            "batch 206: loss 0.104370\n",
            "batch 207: loss 0.165268\n",
            "batch 208: loss 0.053743\n",
            "batch 209: loss 0.140979\n",
            "batch 210: loss 0.028946\n",
            "batch 211: loss 0.061982\n",
            "batch 212: loss 0.038331\n",
            "batch 213: loss 0.304311\n",
            "batch 214: loss 0.045362\n",
            "batch 215: loss 0.025390\n",
            "batch 216: loss 0.146281\n",
            "batch 217: loss 0.063836\n",
            "batch 218: loss 0.069641\n",
            "batch 219: loss 0.055546\n",
            "batch 220: loss 0.097406\n",
            "batch 221: loss 0.300177\n",
            "batch 222: loss 0.079674\n",
            "batch 223: loss 0.034818\n",
            "batch 224: loss 0.115321\n",
            "batch 225: loss 0.081214\n",
            "batch 226: loss 0.117922\n",
            "batch 227: loss 0.104016\n",
            "batch 228: loss 0.066347\n",
            "batch 229: loss 0.066040\n",
            "batch 230: loss 0.020874\n",
            "batch 231: loss 0.050476\n",
            "batch 232: loss 0.138964\n",
            "batch 233: loss 0.117311\n",
            "batch 234: loss 0.147224\n",
            "batch 235: loss 0.185664\n",
            "batch 236: loss 0.092529\n",
            "batch 237: loss 0.030943\n",
            "batch 238: loss 0.132979\n",
            "batch 239: loss 0.033551\n",
            "batch 240: loss 0.220327\n",
            "batch 241: loss 0.086824\n",
            "batch 242: loss 0.189610\n",
            "batch 243: loss 0.024035\n",
            "batch 244: loss 0.124728\n",
            "batch 245: loss 0.020717\n",
            "batch 246: loss 0.191011\n",
            "batch 247: loss 0.029940\n",
            "batch 248: loss 0.052643\n",
            "batch 249: loss 0.248179\n",
            "batch 250: loss 0.098936\n",
            "batch 251: loss 0.126167\n",
            "batch 252: loss 0.214949\n",
            "batch 253: loss 0.100222\n",
            "batch 254: loss 0.293130\n",
            "batch 255: loss 0.060657\n",
            "batch 256: loss 0.087070\n",
            "batch 257: loss 0.053608\n",
            "batch 258: loss 0.122179\n",
            "batch 259: loss 0.125751\n",
            "batch 260: loss 0.026119\n",
            "batch 261: loss 0.042432\n",
            "batch 262: loss 0.094934\n",
            "batch 263: loss 0.037520\n",
            "batch 264: loss 0.070744\n",
            "batch 265: loss 0.096554\n",
            "batch 266: loss 0.053171\n",
            "batch 267: loss 0.082619\n",
            "batch 268: loss 0.130614\n",
            "batch 269: loss 0.041240\n",
            "batch 270: loss 0.054190\n",
            "batch 271: loss 0.478095\n",
            "batch 272: loss 0.018562\n",
            "batch 273: loss 0.065375\n",
            "batch 274: loss 0.041823\n",
            "batch 275: loss 0.169358\n",
            "batch 276: loss 0.063308\n",
            "batch 277: loss 0.130912\n",
            "batch 278: loss 0.085618\n",
            "batch 279: loss 0.135651\n",
            "batch 280: loss 0.294997\n",
            "batch 281: loss 0.079235\n",
            "batch 282: loss 0.083011\n",
            "batch 283: loss 0.198009\n",
            "batch 284: loss 0.170371\n",
            "batch 285: loss 0.041296\n",
            "batch 286: loss 0.175136\n",
            "batch 287: loss 0.134558\n",
            "batch 288: loss 0.155341\n",
            "batch 289: loss 0.143825\n",
            "batch 290: loss 0.037706\n",
            "batch 291: loss 0.028233\n",
            "batch 292: loss 0.126646\n",
            "batch 293: loss 0.056219\n",
            "batch 294: loss 0.184755\n",
            "batch 295: loss 0.039975\n",
            "batch 296: loss 0.143030\n",
            "batch 297: loss 0.066931\n",
            "batch 298: loss 0.073451\n",
            "batch 299: loss 0.025758\n",
            "batch 300: loss 0.074003\n",
            "batch 301: loss 0.251763\n",
            "batch 302: loss 0.105005\n",
            "batch 303: loss 0.069833\n",
            "batch 304: loss 0.035643\n",
            "batch 305: loss 0.022680\n",
            "batch 306: loss 0.038371\n",
            "batch 307: loss 0.091841\n",
            "batch 308: loss 0.012822\n",
            "batch 309: loss 0.022553\n",
            "batch 310: loss 0.088999\n",
            "batch 311: loss 0.089147\n",
            "batch 312: loss 0.030779\n",
            "batch 313: loss 0.072873\n",
            "batch 314: loss 0.018445\n",
            "batch 315: loss 0.112700\n",
            "batch 316: loss 0.012905\n",
            "batch 317: loss 0.057881\n",
            "batch 318: loss 0.073245\n",
            "batch 319: loss 0.020861\n",
            "batch 320: loss 0.087968\n",
            "batch 321: loss 0.167182\n",
            "batch 322: loss 0.041667\n",
            "batch 323: loss 0.095619\n",
            "batch 324: loss 0.014222\n",
            "batch 325: loss 0.255844\n",
            "batch 326: loss 0.091119\n",
            "batch 327: loss 0.036559\n",
            "batch 328: loss 0.066455\n",
            "batch 329: loss 0.070380\n",
            "batch 330: loss 0.004420\n",
            "batch 331: loss 0.063109\n",
            "batch 332: loss 0.171755\n",
            "batch 333: loss 0.248075\n",
            "batch 334: loss 0.024264\n",
            "batch 335: loss 0.078884\n",
            "batch 336: loss 0.079398\n",
            "batch 337: loss 0.069575\n",
            "batch 338: loss 0.041363\n",
            "batch 339: loss 0.072241\n",
            "batch 340: loss 0.112368\n",
            "batch 341: loss 0.114773\n",
            "batch 342: loss 0.061432\n",
            "batch 343: loss 0.169014\n",
            "batch 344: loss 0.008472\n",
            "batch 345: loss 0.029125\n",
            "batch 346: loss 0.030668\n",
            "batch 347: loss 0.019268\n",
            "batch 348: loss 0.021805\n",
            "batch 349: loss 0.059086\n",
            "batch 350: loss 0.048696\n",
            "batch 351: loss 0.043272\n",
            "batch 352: loss 0.065811\n",
            "batch 353: loss 0.145958\n",
            "batch 354: loss 0.126836\n",
            "batch 355: loss 0.139245\n",
            "batch 356: loss 0.107812\n",
            "batch 357: loss 0.025231\n",
            "batch 358: loss 0.044598\n",
            "batch 359: loss 0.046240\n",
            "batch 360: loss 0.237084\n",
            "batch 361: loss 0.076241\n",
            "batch 362: loss 0.027908\n",
            "batch 363: loss 0.300583\n",
            "batch 364: loss 0.013128\n",
            "batch 365: loss 0.012083\n",
            "batch 366: loss 0.100225\n",
            "batch 367: loss 0.166613\n",
            "batch 368: loss 0.060914\n",
            "batch 369: loss 0.028033\n",
            "batch 370: loss 0.031572\n",
            "batch 371: loss 0.029513\n",
            "batch 372: loss 0.056709\n",
            "batch 373: loss 0.057219\n",
            "batch 374: loss 0.079235\n",
            "batch 375: loss 0.097048\n",
            "batch 376: loss 0.117602\n",
            "batch 377: loss 0.049266\n",
            "batch 378: loss 0.154139\n",
            "batch 379: loss 0.027138\n",
            "batch 380: loss 0.041612\n",
            "batch 381: loss 0.069107\n",
            "batch 382: loss 0.078587\n",
            "batch 383: loss 0.079760\n",
            "batch 384: loss 0.111924\n",
            "batch 385: loss 0.032554\n",
            "batch 386: loss 0.061809\n",
            "batch 387: loss 0.021912\n",
            "batch 388: loss 0.011511\n",
            "batch 389: loss 0.050812\n",
            "batch 390: loss 0.015113\n",
            "batch 391: loss 0.103020\n",
            "batch 392: loss 0.021140\n",
            "batch 393: loss 0.034168\n",
            "batch 394: loss 0.072419\n",
            "batch 395: loss 0.007170\n",
            "batch 396: loss 0.091170\n",
            "batch 397: loss 0.114147\n",
            "batch 398: loss 0.125947\n",
            "batch 399: loss 0.050815\n",
            "batch 400: loss 0.103278\n",
            "batch 401: loss 0.012192\n",
            "batch 402: loss 0.071828\n",
            "batch 403: loss 0.081581\n",
            "batch 404: loss 0.025703\n",
            "batch 405: loss 0.085030\n",
            "batch 406: loss 0.024293\n",
            "batch 407: loss 0.014059\n",
            "batch 408: loss 0.131299\n",
            "batch 409: loss 0.186048\n",
            "batch 410: loss 0.001674\n",
            "batch 411: loss 0.072826\n",
            "batch 412: loss 0.028250\n",
            "batch 413: loss 0.039494\n",
            "batch 414: loss 0.038930\n",
            "batch 415: loss 0.124024\n",
            "batch 416: loss 0.183628\n",
            "batch 417: loss 0.152948\n",
            "batch 418: loss 0.045348\n",
            "batch 419: loss 0.133156\n",
            "batch 420: loss 0.029830\n",
            "batch 421: loss 0.011102\n",
            "batch 422: loss 0.066639\n",
            "batch 423: loss 0.091018\n",
            "batch 424: loss 0.094230\n",
            "batch 425: loss 0.011319\n",
            "batch 426: loss 0.010494\n",
            "batch 427: loss 0.091507\n",
            "batch 428: loss 0.219144\n",
            "batch 429: loss 0.033685\n",
            "batch 430: loss 0.078202\n",
            "batch 431: loss 0.076045\n",
            "batch 432: loss 0.026047\n",
            "batch 433: loss 0.022197\n",
            "batch 434: loss 0.039877\n",
            "batch 435: loss 0.225676\n",
            "batch 436: loss 0.023536\n",
            "batch 437: loss 0.135395\n",
            "batch 438: loss 0.114004\n",
            "batch 439: loss 0.010853\n",
            "batch 440: loss 0.016531\n",
            "batch 441: loss 0.172027\n",
            "batch 442: loss 0.011951\n",
            "batch 443: loss 0.012520\n",
            "batch 444: loss 0.040553\n",
            "batch 445: loss 0.015051\n",
            "batch 446: loss 0.027735\n",
            "batch 447: loss 0.021699\n",
            "batch 448: loss 0.090877\n",
            "batch 449: loss 0.075337\n",
            "batch 450: loss 0.014742\n",
            "batch 451: loss 0.029880\n",
            "batch 452: loss 0.053013\n",
            "batch 453: loss 0.044899\n",
            "batch 454: loss 0.080737\n",
            "batch 455: loss 0.097535\n",
            "batch 456: loss 0.152869\n",
            "batch 457: loss 0.041813\n",
            "batch 458: loss 0.017089\n",
            "batch 459: loss 0.012616\n",
            "batch 460: loss 0.005339\n",
            "batch 461: loss 0.100981\n",
            "batch 462: loss 0.040529\n",
            "batch 463: loss 0.054347\n",
            "batch 464: loss 0.227336\n",
            "batch 465: loss 0.027597\n",
            "batch 466: loss 0.022340\n",
            "batch 467: loss 0.035247\n",
            "batch 468: loss 0.012194\n",
            "batch 469: loss 0.014191\n",
            "batch 470: loss 0.021954\n",
            "batch 471: loss 0.011195\n",
            "batch 472: loss 0.050285\n",
            "batch 473: loss 0.042835\n",
            "batch 474: loss 0.117203\n",
            "batch 475: loss 0.005516\n",
            "batch 476: loss 0.077696\n",
            "batch 477: loss 0.063925\n",
            "batch 478: loss 0.028333\n",
            "batch 479: loss 0.089339\n",
            "batch 480: loss 0.008476\n",
            "batch 481: loss 0.185541\n",
            "batch 482: loss 0.018076\n",
            "batch 483: loss 0.059810\n",
            "batch 484: loss 0.152495\n",
            "batch 485: loss 0.131677\n",
            "batch 486: loss 0.016163\n",
            "batch 487: loss 0.017336\n",
            "batch 488: loss 0.008390\n",
            "batch 489: loss 0.059116\n",
            "batch 490: loss 0.021082\n",
            "batch 491: loss 0.205545\n",
            "batch 492: loss 0.309909\n",
            "batch 493: loss 0.057046\n",
            "batch 494: loss 0.072669\n",
            "batch 495: loss 0.021014\n",
            "batch 496: loss 0.034014\n",
            "batch 497: loss 0.103724\n",
            "batch 498: loss 0.115817\n",
            "batch 499: loss 0.017429\n",
            "batch 500: loss 0.036162\n",
            "batch 501: loss 0.030635\n",
            "batch 502: loss 0.053166\n",
            "batch 503: loss 0.083718\n",
            "batch 504: loss 0.005592\n",
            "batch 505: loss 0.002765\n",
            "batch 506: loss 0.035743\n",
            "batch 507: loss 0.011436\n",
            "batch 508: loss 0.066457\n",
            "batch 509: loss 0.035469\n",
            "batch 510: loss 0.014400\n",
            "batch 511: loss 0.033601\n",
            "batch 512: loss 0.007677\n",
            "batch 513: loss 0.050003\n",
            "batch 514: loss 0.297888\n",
            "batch 515: loss 0.051298\n",
            "batch 516: loss 0.025629\n",
            "batch 517: loss 0.166644\n",
            "batch 518: loss 0.004073\n",
            "batch 519: loss 0.042988\n",
            "batch 520: loss 0.032158\n",
            "batch 521: loss 0.027542\n",
            "batch 522: loss 0.043348\n",
            "batch 523: loss 0.112538\n",
            "batch 524: loss 0.016878\n",
            "batch 525: loss 0.037099\n",
            "batch 526: loss 0.033079\n",
            "batch 527: loss 0.063025\n",
            "batch 528: loss 0.048458\n",
            "batch 529: loss 0.024147\n",
            "batch 530: loss 0.020162\n",
            "batch 531: loss 0.099493\n",
            "batch 532: loss 0.097608\n",
            "batch 533: loss 0.020747\n",
            "batch 534: loss 0.060365\n",
            "batch 535: loss 0.069495\n",
            "batch 536: loss 0.070465\n",
            "batch 537: loss 0.006323\n",
            "batch 538: loss 0.075132\n",
            "batch 539: loss 0.067255\n",
            "batch 540: loss 0.094860\n",
            "batch 541: loss 0.013602\n",
            "batch 542: loss 0.033648\n",
            "batch 543: loss 0.044532\n",
            "batch 544: loss 0.198353\n",
            "batch 545: loss 0.134889\n",
            "batch 546: loss 0.008810\n",
            "batch 547: loss 0.002279\n",
            "batch 548: loss 0.157311\n",
            "batch 549: loss 0.023353\n",
            "batch 550: loss 0.026488\n",
            "batch 551: loss 0.258034\n",
            "batch 552: loss 0.044886\n",
            "batch 553: loss 0.143915\n",
            "batch 554: loss 0.114653\n",
            "batch 555: loss 0.004967\n",
            "batch 556: loss 0.100124\n",
            "batch 557: loss 0.051647\n",
            "batch 558: loss 0.095382\n",
            "batch 559: loss 0.095008\n",
            "batch 560: loss 0.022413\n",
            "batch 561: loss 0.107266\n",
            "batch 562: loss 0.042694\n",
            "batch 563: loss 0.077324\n",
            "batch 564: loss 0.032077\n",
            "batch 565: loss 0.165406\n",
            "batch 566: loss 0.015465\n",
            "batch 567: loss 0.112025\n",
            "batch 568: loss 0.037361\n",
            "batch 569: loss 0.070721\n",
            "batch 570: loss 0.044416\n",
            "batch 571: loss 0.081395\n",
            "batch 572: loss 0.023474\n",
            "batch 573: loss 0.021986\n",
            "batch 574: loss 0.075936\n",
            "batch 575: loss 0.016518\n",
            "batch 576: loss 0.002942\n",
            "batch 577: loss 0.040838\n",
            "batch 578: loss 0.029299\n",
            "batch 579: loss 0.072701\n",
            "batch 580: loss 0.007560\n",
            "batch 581: loss 0.083321\n",
            "batch 582: loss 0.049510\n",
            "batch 583: loss 0.004111\n",
            "batch 584: loss 0.121532\n",
            "batch 585: loss 0.018666\n",
            "batch 586: loss 0.020082\n",
            "batch 587: loss 0.021384\n",
            "batch 588: loss 0.060255\n",
            "batch 589: loss 0.017683\n",
            "batch 590: loss 0.012666\n",
            "batch 591: loss 0.110487\n",
            "batch 592: loss 0.047302\n",
            "batch 593: loss 0.008632\n",
            "batch 594: loss 0.238246\n",
            "batch 595: loss 0.031921\n",
            "batch 596: loss 0.048575\n",
            "batch 597: loss 0.016016\n",
            "batch 598: loss 0.090124\n",
            "batch 599: loss 0.051764\n",
            "batch 600: loss 0.125158\n",
            "batch 601: loss 0.027297\n",
            "batch 602: loss 0.008292\n",
            "batch 603: loss 0.059259\n",
            "batch 604: loss 0.089208\n",
            "batch 605: loss 0.004908\n",
            "batch 606: loss 0.009337\n",
            "batch 607: loss 0.210362\n",
            "batch 608: loss 0.012664\n",
            "batch 609: loss 0.003315\n",
            "batch 610: loss 0.015460\n",
            "batch 611: loss 0.027966\n",
            "batch 612: loss 0.005235\n",
            "batch 613: loss 0.160011\n",
            "batch 614: loss 0.050894\n",
            "batch 615: loss 0.004440\n",
            "batch 616: loss 0.002498\n",
            "batch 617: loss 0.218111\n",
            "batch 618: loss 0.004007\n",
            "batch 619: loss 0.022228\n",
            "batch 620: loss 0.038009\n",
            "batch 621: loss 0.022216\n",
            "batch 622: loss 0.020744\n",
            "batch 623: loss 0.046736\n",
            "batch 624: loss 0.015595\n",
            "batch 625: loss 0.044361\n",
            "batch 626: loss 0.124955\n",
            "batch 627: loss 0.030340\n",
            "batch 628: loss 0.200160\n",
            "batch 629: loss 0.008176\n",
            "batch 630: loss 0.133184\n",
            "batch 631: loss 0.182243\n",
            "batch 632: loss 0.006847\n",
            "batch 633: loss 0.006793\n",
            "batch 634: loss 0.012189\n",
            "batch 635: loss 0.010490\n",
            "batch 636: loss 0.004825\n",
            "batch 637: loss 0.121738\n",
            "batch 638: loss 0.030293\n",
            "batch 639: loss 0.007162\n",
            "batch 640: loss 0.074581\n",
            "batch 641: loss 0.009228\n",
            "batch 642: loss 0.010756\n",
            "batch 643: loss 0.037427\n",
            "batch 644: loss 0.156262\n",
            "batch 645: loss 0.060287\n",
            "batch 646: loss 0.017939\n",
            "batch 647: loss 0.015691\n",
            "batch 648: loss 0.035581\n",
            "batch 649: loss 0.126760\n",
            "batch 650: loss 0.016332\n",
            "batch 651: loss 0.016371\n",
            "batch 652: loss 0.040232\n",
            "batch 653: loss 0.087772\n",
            "batch 654: loss 0.019117\n",
            "batch 655: loss 0.176680\n",
            "batch 656: loss 0.026617\n",
            "batch 657: loss 0.004499\n",
            "batch 658: loss 0.012308\n",
            "batch 659: loss 0.030535\n",
            "batch 660: loss 0.007809\n",
            "batch 661: loss 0.133778\n",
            "batch 662: loss 0.016824\n",
            "batch 663: loss 0.020782\n",
            "batch 664: loss 0.030449\n",
            "batch 665: loss 0.003976\n",
            "batch 666: loss 0.008908\n",
            "batch 667: loss 0.288804\n",
            "batch 668: loss 0.006717\n",
            "batch 669: loss 0.188099\n",
            "batch 670: loss 0.047884\n",
            "batch 671: loss 0.005875\n",
            "batch 672: loss 0.024634\n",
            "batch 673: loss 0.008432\n",
            "batch 674: loss 0.055611\n",
            "batch 675: loss 0.031311\n",
            "batch 676: loss 0.012673\n",
            "batch 677: loss 0.022793\n",
            "batch 678: loss 0.107160\n",
            "batch 679: loss 0.028170\n",
            "batch 680: loss 0.011414\n",
            "batch 681: loss 0.086593\n",
            "batch 682: loss 0.131720\n",
            "batch 683: loss 0.005363\n",
            "batch 684: loss 0.015260\n",
            "batch 685: loss 0.008384\n",
            "batch 686: loss 0.028066\n",
            "batch 687: loss 0.122091\n",
            "batch 688: loss 0.018178\n",
            "batch 689: loss 0.131715\n",
            "batch 690: loss 0.029012\n",
            "batch 691: loss 0.015500\n",
            "batch 692: loss 0.037044\n",
            "batch 693: loss 0.007144\n",
            "batch 694: loss 0.055110\n",
            "batch 695: loss 0.059258\n",
            "batch 696: loss 0.002076\n",
            "batch 697: loss 0.042747\n",
            "batch 698: loss 0.044009\n",
            "batch 699: loss 0.075086\n",
            "batch 700: loss 0.005996\n",
            "batch 701: loss 0.003375\n",
            "batch 702: loss 0.155903\n",
            "batch 703: loss 0.019729\n",
            "batch 704: loss 0.032384\n",
            "batch 705: loss 0.056666\n",
            "batch 706: loss 0.026227\n",
            "batch 707: loss 0.008319\n",
            "batch 708: loss 0.008516\n",
            "batch 709: loss 0.055617\n",
            "batch 710: loss 0.243005\n",
            "batch 711: loss 0.011603\n",
            "batch 712: loss 0.085837\n",
            "batch 713: loss 0.070097\n",
            "batch 714: loss 0.014073\n",
            "batch 715: loss 0.005973\n",
            "batch 716: loss 0.010844\n",
            "batch 717: loss 0.009617\n",
            "batch 718: loss 0.146104\n",
            "batch 719: loss 0.006950\n",
            "batch 720: loss 0.032200\n",
            "batch 721: loss 0.004124\n",
            "batch 722: loss 0.047430\n",
            "batch 723: loss 0.247088\n",
            "batch 724: loss 0.044671\n",
            "batch 725: loss 0.047280\n",
            "batch 726: loss 0.070206\n",
            "batch 727: loss 0.066592\n",
            "batch 728: loss 0.105587\n",
            "batch 729: loss 0.003552\n",
            "batch 730: loss 0.035525\n",
            "batch 731: loss 0.058999\n",
            "batch 732: loss 0.038517\n",
            "batch 733: loss 0.033733\n",
            "batch 734: loss 0.028329\n",
            "batch 735: loss 0.025969\n",
            "batch 736: loss 0.009682\n",
            "batch 737: loss 0.167003\n",
            "batch 738: loss 0.012915\n",
            "batch 739: loss 0.023371\n",
            "batch 740: loss 0.182006\n",
            "batch 741: loss 0.081151\n",
            "batch 742: loss 0.070769\n",
            "batch 743: loss 0.048376\n",
            "batch 744: loss 0.084319\n",
            "batch 745: loss 0.014035\n",
            "batch 746: loss 0.052367\n",
            "batch 747: loss 0.063000\n",
            "batch 748: loss 0.011218\n",
            "batch 749: loss 0.026180\n",
            "batch 750: loss 0.072251\n",
            "batch 751: loss 0.127712\n",
            "batch 752: loss 0.064041\n",
            "batch 753: loss 0.021340\n",
            "batch 754: loss 0.075293\n",
            "batch 755: loss 0.032086\n",
            "batch 756: loss 0.014912\n",
            "batch 757: loss 0.033324\n",
            "batch 758: loss 0.009450\n",
            "batch 759: loss 0.065452\n",
            "batch 760: loss 0.019026\n",
            "batch 761: loss 0.011366\n",
            "batch 762: loss 0.028659\n",
            "batch 763: loss 0.012191\n",
            "batch 764: loss 0.045470\n",
            "batch 765: loss 0.211088\n",
            "batch 766: loss 0.055889\n",
            "batch 767: loss 0.066110\n",
            "batch 768: loss 0.015903\n",
            "batch 769: loss 0.021143\n",
            "batch 770: loss 0.065873\n",
            "batch 771: loss 0.005398\n",
            "batch 772: loss 0.149541\n",
            "batch 773: loss 0.015015\n",
            "batch 774: loss 0.037991\n",
            "batch 775: loss 0.099280\n",
            "batch 776: loss 0.008274\n",
            "batch 777: loss 0.020720\n",
            "batch 778: loss 0.154235\n",
            "batch 779: loss 0.230303\n",
            "batch 780: loss 0.009548\n",
            "batch 781: loss 0.072430\n",
            "batch 782: loss 0.018135\n",
            "batch 783: loss 0.010533\n",
            "batch 784: loss 0.023134\n",
            "batch 785: loss 0.072998\n",
            "batch 786: loss 0.017619\n",
            "batch 787: loss 0.062864\n",
            "batch 788: loss 0.085605\n",
            "batch 789: loss 0.143069\n",
            "batch 790: loss 0.086945\n",
            "batch 791: loss 0.080440\n",
            "batch 792: loss 0.116616\n",
            "batch 793: loss 0.055696\n",
            "batch 794: loss 0.003383\n",
            "batch 795: loss 0.015437\n",
            "batch 796: loss 0.005497\n",
            "batch 797: loss 0.009608\n",
            "batch 798: loss 0.110179\n",
            "batch 799: loss 0.016057\n",
            "batch 800: loss 0.013214\n",
            "batch 801: loss 0.093117\n",
            "batch 802: loss 0.013317\n",
            "batch 803: loss 0.007156\n",
            "batch 804: loss 0.082261\n",
            "batch 805: loss 0.056371\n",
            "batch 806: loss 0.078709\n",
            "batch 807: loss 0.185907\n",
            "batch 808: loss 0.151073\n",
            "batch 809: loss 0.111002\n",
            "batch 810: loss 0.131028\n",
            "batch 811: loss 0.010665\n",
            "batch 812: loss 0.015081\n",
            "batch 813: loss 0.036864\n",
            "batch 814: loss 0.011634\n",
            "batch 815: loss 0.014136\n",
            "batch 816: loss 0.066951\n",
            "batch 817: loss 0.026135\n",
            "batch 818: loss 0.008197\n",
            "batch 819: loss 0.068249\n",
            "batch 820: loss 0.015187\n",
            "batch 821: loss 0.157362\n",
            "batch 822: loss 0.036397\n",
            "batch 823: loss 0.028140\n",
            "batch 824: loss 0.012455\n",
            "batch 825: loss 0.058100\n",
            "batch 826: loss 0.016256\n",
            "batch 827: loss 0.008902\n",
            "batch 828: loss 0.035406\n",
            "batch 829: loss 0.015947\n",
            "batch 830: loss 0.070073\n",
            "batch 831: loss 0.004602\n",
            "batch 832: loss 0.005868\n",
            "batch 833: loss 0.014845\n",
            "batch 834: loss 0.112462\n",
            "batch 835: loss 0.043359\n",
            "batch 836: loss 0.021824\n",
            "batch 837: loss 0.003786\n",
            "batch 838: loss 0.135299\n",
            "batch 839: loss 0.012890\n",
            "batch 840: loss 0.030134\n",
            "batch 841: loss 0.024326\n",
            "batch 842: loss 0.042360\n",
            "batch 843: loss 0.065074\n",
            "batch 844: loss 0.024202\n",
            "batch 845: loss 0.093769\n",
            "batch 846: loss 0.011834\n",
            "batch 847: loss 0.016193\n",
            "batch 848: loss 0.067146\n",
            "batch 849: loss 0.013135\n",
            "batch 850: loss 0.030759\n",
            "batch 851: loss 0.041729\n",
            "batch 852: loss 0.080343\n",
            "batch 853: loss 0.046142\n",
            "batch 854: loss 0.032687\n",
            "batch 855: loss 0.003193\n",
            "batch 856: loss 0.058908\n",
            "batch 857: loss 0.020912\n",
            "batch 858: loss 0.019388\n",
            "batch 859: loss 0.035704\n",
            "batch 860: loss 0.008745\n",
            "batch 861: loss 0.036417\n",
            "batch 862: loss 0.033691\n",
            "batch 863: loss 0.031788\n",
            "batch 864: loss 0.091056\n",
            "batch 865: loss 0.123016\n",
            "batch 866: loss 0.004881\n",
            "batch 867: loss 0.217262\n",
            "batch 868: loss 0.222827\n",
            "batch 869: loss 0.016481\n",
            "batch 870: loss 0.013624\n",
            "batch 871: loss 0.020647\n",
            "batch 872: loss 0.070564\n",
            "batch 873: loss 0.002222\n",
            "batch 874: loss 0.073792\n",
            "batch 875: loss 0.037932\n",
            "batch 876: loss 0.015976\n",
            "batch 877: loss 0.046594\n",
            "batch 878: loss 0.011106\n",
            "batch 879: loss 0.006281\n",
            "batch 880: loss 0.028510\n",
            "batch 881: loss 0.069399\n",
            "batch 882: loss 0.243504\n",
            "batch 883: loss 0.030216\n",
            "batch 884: loss 0.121814\n",
            "batch 885: loss 0.002048\n",
            "batch 886: loss 0.027356\n",
            "batch 887: loss 0.063829\n",
            "batch 888: loss 0.187809\n",
            "batch 889: loss 0.065204\n",
            "batch 890: loss 0.005702\n",
            "batch 891: loss 0.000742\n",
            "batch 892: loss 0.062404\n",
            "batch 893: loss 0.012807\n",
            "batch 894: loss 0.035666\n",
            "batch 895: loss 0.013657\n",
            "batch 896: loss 0.038069\n",
            "batch 897: loss 0.127393\n",
            "batch 898: loss 0.054789\n",
            "batch 899: loss 0.018348\n",
            "batch 900: loss 0.007568\n",
            "batch 901: loss 0.008577\n",
            "batch 902: loss 0.142985\n",
            "batch 903: loss 0.187990\n",
            "batch 904: loss 0.012167\n",
            "batch 905: loss 0.006927\n",
            "batch 906: loss 0.066448\n",
            "batch 907: loss 0.011395\n",
            "batch 908: loss 0.069332\n",
            "batch 909: loss 0.063156\n",
            "batch 910: loss 0.014327\n",
            "batch 911: loss 0.007473\n",
            "batch 912: loss 0.005437\n",
            "batch 913: loss 0.065618\n",
            "batch 914: loss 0.010772\n",
            "batch 915: loss 0.006409\n",
            "batch 916: loss 0.086233\n",
            "batch 917: loss 0.025048\n",
            "batch 918: loss 0.011522\n",
            "batch 919: loss 0.065587\n",
            "batch 920: loss 0.000532\n",
            "batch 921: loss 0.060237\n",
            "batch 922: loss 0.048728\n",
            "batch 923: loss 0.023204\n",
            "batch 924: loss 0.067679\n",
            "batch 925: loss 0.094457\n",
            "batch 926: loss 0.231145\n",
            "batch 927: loss 0.011505\n",
            "batch 928: loss 0.008562\n",
            "batch 929: loss 0.016637\n",
            "batch 930: loss 0.014331\n",
            "batch 931: loss 0.086067\n",
            "batch 932: loss 0.031322\n",
            "batch 933: loss 0.015462\n",
            "batch 934: loss 0.004391\n",
            "batch 935: loss 0.005136\n",
            "batch 936: loss 0.026989\n",
            "batch 937: loss 0.028458\n",
            "batch 938: loss 0.011775\n",
            "batch 939: loss 0.032739\n",
            "batch 940: loss 0.016930\n",
            "batch 941: loss 0.001753\n",
            "batch 942: loss 0.125778\n",
            "batch 943: loss 0.008207\n",
            "batch 944: loss 0.015660\n",
            "batch 945: loss 0.112628\n",
            "batch 946: loss 0.130477\n",
            "batch 947: loss 0.042347\n",
            "batch 948: loss 0.000870\n",
            "batch 949: loss 0.022348\n",
            "batch 950: loss 0.060214\n",
            "batch 951: loss 0.023131\n",
            "batch 952: loss 0.019019\n",
            "batch 953: loss 0.142879\n",
            "batch 954: loss 0.044391\n",
            "batch 955: loss 0.022563\n",
            "batch 956: loss 0.059076\n",
            "batch 957: loss 0.017177\n",
            "batch 958: loss 0.231150\n",
            "batch 959: loss 0.009890\n",
            "batch 960: loss 0.181483\n",
            "batch 961: loss 0.012553\n",
            "batch 962: loss 0.090750\n",
            "batch 963: loss 0.039633\n",
            "batch 964: loss 0.019632\n",
            "batch 965: loss 0.064213\n",
            "batch 966: loss 0.016229\n",
            "batch 967: loss 0.008112\n",
            "batch 968: loss 0.005751\n",
            "batch 969: loss 0.035368\n",
            "batch 970: loss 0.035873\n",
            "batch 971: loss 0.010023\n",
            "batch 972: loss 0.034522\n",
            "batch 973: loss 0.078791\n",
            "batch 974: loss 0.051566\n",
            "batch 975: loss 0.015826\n",
            "batch 976: loss 0.013929\n",
            "batch 977: loss 0.015094\n",
            "batch 978: loss 0.030999\n",
            "batch 979: loss 0.004559\n",
            "batch 980: loss 0.140528\n",
            "batch 981: loss 0.028298\n",
            "batch 982: loss 0.004772\n",
            "batch 983: loss 0.128243\n",
            "batch 984: loss 0.161500\n",
            "batch 985: loss 0.098089\n",
            "batch 986: loss 0.017489\n",
            "batch 987: loss 0.008684\n",
            "batch 988: loss 0.078151\n",
            "batch 989: loss 0.032722\n",
            "batch 990: loss 0.061308\n",
            "batch 991: loss 0.110036\n",
            "batch 992: loss 0.045679\n",
            "batch 993: loss 0.073055\n",
            "batch 994: loss 0.016701\n",
            "batch 995: loss 0.007869\n",
            "batch 996: loss 0.110429\n",
            "batch 997: loss 0.014501\n",
            "batch 998: loss 0.053974\n",
            "batch 999: loss 0.027433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zgpulW8J2qq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20de3e94-0afb-41ef-928d-c32917b4a136"
      },
      "cell_type": "code",
      "source": [
        "num_eval_samples = np.shape(data_loader.eval_labels)[0]\n",
        "y_pred = model.predict(data_loader.eval_data).numpy()\n",
        "print(\"test accuracy: %f\" % (sum(y_pred == data_loader.eval_labels) / num_eval_samples))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 0.986200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ea52c03x2qto",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KaP7fc272qwf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LWfMGC9E2qzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4pmqRal2q1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}